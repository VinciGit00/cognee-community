{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc452f1e",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Imports core libs (async, I/O, dates) plus `cognee`, `praw` (Reddit), and `feedparser`.  \n",
    "Loads secrets from `.env` to configure APIs and runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîê Environment Variables Setup\n",
    "\n",
    "# TAVILY_API_KEY=your_tavily_api_key_here\n",
    "# REDDIT_CLIENT_ID=your_reddit_client_id_here\n",
    "# REDDIT_CLIENT_SECRET=your_reddit_client_secret_here\n",
    "# REDDIT_USER_AGENT=your_app_name_or_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5cf9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m2025-11-13T16:22:32.343750\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDeleted old log file: /opt/homebrew/lib/python3.11/site-packages/logs/2025-10-29_12-28-05.log\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:22:32.686294\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mLogging initialized           \u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m \u001b[36mcognee_version\u001b[0m=\u001b[35m0.3.6\u001b[0m \u001b[36mdatabase_path\u001b[0m=\u001b[35m/opt/homebrew/lib/python3.11/site-packages/cognee/.cognee_system/databases\u001b[0m \u001b[36mgraph_database_name\u001b[0m=\u001b[35m\u001b[0m \u001b[36mos_info\u001b[0m=\u001b[35m'Darwin 24.6.0 (Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:40 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T8132)'\u001b[0m \u001b[36mpython_version\u001b[0m=\u001b[35m3.11.13\u001b[0m \u001b[36mrelational_config\u001b[0m=\u001b[35mcognee_db\u001b[0m \u001b[36mstructlog_version\u001b[0m=\u001b[35m25.4.0\u001b[0m \u001b[36mvector_config\u001b[0m=\u001b[35mlancedb\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:22:32.687450\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDatabase storage: /opt/homebrew/lib/python3.11/site-packages/cognee/.cognee_system/databases\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:22:33.463162\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1munstructured[pdf] not installed, can't use AdvancedPdfLoader, will use PyPdfLoader instead.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.infrastructure.loaders.external.advanced_pdf_loader\u001b[0m]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import asyncio\n",
    "import os\n",
    "import cognee\n",
    "import praw\n",
    "import tempfile\n",
    "import feedparser\n",
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e7f3e",
   "metadata": {},
   "source": [
    "### Clean Slate\n",
    "Wipes previously stored documents, nodes, and relationships. Only use this when you want a fresh/ cleaned memory\n",
    "Clears system-level caches/metadata (indexes, config history) to ensure a fresh run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55c585b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m2025-11-13T16:22:33.682749\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mLoaded JSON extension         \u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:22:33.701635\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDeleted Kuzu database files at /opt/homebrew/lib/python3.11/site-packages/cognee/.cognee_system/databases/cognee_graph_kuzu\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pruned.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m2025-11-13T16:22:36.363323\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDatabase deleted successfully.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[1mStorage manager absolute path: /opt/homebrew/lib/python3.11/site-packages/cognee/.cognee_cache\u001b[0m\n",
      "\n",
      "\u001b[1mDeleting cache...             \u001b[0m\n",
      "\n",
      "\u001b[1m‚úì Cache deleted successfully! \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata pruned.\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # delete stored data (documents, nodes, relationships)\n",
    "    await cognee.prune.prune_data()\n",
    "    print(\"Data pruned.\")\n",
    "\n",
    "    # clear system-level caches and metadata (indexes, config history, etc.)\n",
    "    await cognee.prune.prune_system(metadata=True)\n",
    "    print(\"Metadata pruned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca77d42",
   "metadata": {},
   "source": [
    "### Ingest Reddit\n",
    "Initializes Reddit API via PRAW using env credentials.  \n",
    "Fetches latest posts from selected subreddits, writes them to temp files.  \n",
    "Adds those files to Cognee for ingestion, then removes the temp files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ab06d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit API initialized.\n",
      "Fetching posts from r/OpenAI...\n",
      "  ‚úì Fetched and saved posts from r/OpenAI\n",
      "User 88e14804-92ba-4b99-81ab-6af593dadffe has registered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mEmbeddingRateLimiter initialized: enabled=False, requests_limit=60, interval_seconds=60\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:02.639914\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mPipeline run started: `0e01cb9b-43ca-5d79-8c4b-1e967f899ad5`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_with_telemetry()\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:02.871485\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `resolve_data_directories`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:03.052900\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `ingest_data`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:03.255469\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRegistered loader: pypdf_loader\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.infrastructure.loaders.LoaderEngine\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:03.256150\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRegistered loader: text_loader\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.infrastructure.loaders.LoaderEngine\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:03.256621\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRegistered loader: image_loader\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.infrastructure.loaders.LoaderEngine\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:03.257042\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRegistered loader: audio_loader\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.infrastructure.loaders.LoaderEngine\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:03.257528\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mRegistered loader: unstructured_loader\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.infrastructure.loaders.LoaderEngine\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:03.266205\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `ingest_data`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:03.438284\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `resolve_data_directories`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:03.627351\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mPipeline run completed: `0e01cb9b-43ca-5d79-8c4b-1e967f899ad5`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_with_telemetry()\u001b[0m]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Reddit posts added to cognee\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    #########################################################\n",
    "    # Add reddit data to cognee pipeline\n",
    "    #########################################################\n",
    "\n",
    "    # --- Initialize Reddit API ---\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "        client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "        user_agent=os.getenv(\"REDDIT_USER_AGENT\", \"stay_on_top_of_news/1.0\")\n",
    "    )\n",
    "\n",
    "    print(\"Reddit API initialized.\")\n",
    "\n",
    "\n",
    "    async def get_reddit_posts(subreddit_name, limit=25):\n",
    "        \"\"\"Fetch recent posts from a subreddit\"\"\"\n",
    "        posts_text = []\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        for post in subreddit.new(limit=limit):\n",
    "            # Evaluate catchiness using LLM (3 lines)            \n",
    "            content = post.selftext if post.selftext else '[Link post - no text content]'\n",
    "            prompt = f\"Rate this news catchiness (1-10). Catchiness = controversial + attention-capturing + timeless + major breakthrough.\\n\\nTitle: {post.title}\\nSummary: {content[:500]}\\n\\nReturn ONLY integer 1-10.\"\n",
    "            \n",
    "            client = AsyncOpenAI(api_key=os.getenv(\"LLM_API_KEY\"), timeout=5.0)\n",
    "            resp = await client.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": prompt}], max_tokens=5)\n",
    "            score = int(resp.choices[0].message.content.strip())\n",
    "                        \n",
    "            post_content = f\"\"\"\n",
    "                            Title: {post.title}\n",
    "                            Subreddit: r/{subreddit_name}\n",
    "                            Author: u/{post.author.name if post.author else '[deleted]'}\n",
    "                            Score: {post.score}\n",
    "                            Created: {datetime.fromtimestamp(post.created_utc)}\n",
    "                            URL: {post.url}\n",
    "                            Catchiness Score: {score}/10\n",
    "\n",
    "                            Content:\n",
    "                            {content}\n",
    "\n",
    "                            ---\n",
    "                            \"\"\"\n",
    "            posts_text.append(post_content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return \"\\n\".join(posts_text)\n",
    "\n",
    "\n",
    "    # Create temporary files for each subreddit's posts and define subreddits\n",
    "    subreddits = [\n",
    "        # \"MachineLearning\",\n",
    "        \"OpenAI\",\n",
    "        # \"artificial\",\n",
    "        # \"LocalLLaMA\",\n",
    "        # \"AIMemory\",\n",
    "        # \"GraphRAG\",\n",
    "        # \"ollama\",\n",
    "        # \"LLMDevs\",\n",
    "        # \"Rag\",\n",
    "        # \"datascience\",\n",
    "        # \"ai\",\n",
    "        # \"ArtificialInteligence\", \n",
    "        # \"deepLearning\",\n",
    "        # \"AI_Agents\",\n",
    "        # \"ChatGPT\",\n",
    "        # \"Singularity\",\n",
    "        # \"StableDiffusion\",\n",
    "        # \"Midjourney\",\n",
    "        # \"generative\",\n",
    "        # \"aipromptprogramming\",\n",
    "        # \"aiart\"\n",
    "    ]\n",
    "    temp_files = []\n",
    "\n",
    "    for sub in subreddits:\n",
    "        try:\n",
    "            print(f\"Fetching posts from r/{sub}...\")\n",
    "            posts_content = await get_reddit_posts(sub, limit=25)\n",
    "            \n",
    "            # Create a temporary file with the posts\n",
    "            temp_file = tempfile.NamedTemporaryFile(\n",
    "                mode='w', \n",
    "                suffix=f'_{sub}_posts.txt',\n",
    "                delete=False,\n",
    "                encoding='utf-8'\n",
    "            )\n",
    "            temp_file.write(posts_content)\n",
    "            temp_file.close()\n",
    "            temp_files.append(temp_file.name)\n",
    "            print(f\"  ‚úì Fetched and saved posts from r/{sub}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Error fetching r/{sub}: {type(e).__name__} - {str(e)}\")\n",
    "            print(f\"  ‚Üí Skipping subreddit and continuing...\")\n",
    "\n",
    "\n",
    "    # --- Add Reddit posts to cognee ---\n",
    "    await cognee.add(temp_files)\n",
    "    print(\"‚úì Reddit posts added to cognee\")\n",
    "\n",
    "    # Clean up temporary files\n",
    "    for temp_file in temp_files:\n",
    "        try:\n",
    "            Path(temp_file).unlink()\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d418f3",
   "metadata": {},
   "source": [
    "### Ingest RSS News\n",
    "Defines a small set of AI/tech RSS feeds.  \n",
    "Parses each feed, formats the latest entries, saves to temp text files.  \n",
    "Adds all parsed feeds to Cognee and cleans up the temp files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd8fa37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing 2 RSS feeds...\n",
      "  Parsing https://export.arxiv.org/rss/cs.LG...\n",
      "  ‚úì Saved\n",
      "  Parsing https://openai.com/news/rss.xml...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m2025-11-13T16:23:17.527875\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mPipeline run started: `0e01cb9b-43ca-5d79-8c4b-1e967f899ad5`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_with_telemetry()\u001b[0m]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Saved\n",
      "\n",
      "Adding RSS feeds to cognee...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m2025-11-13T16:23:17.699601\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `resolve_data_directories`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:17.874630\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `ingest_data`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:18.058720\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mPipeline run started: `0e01cb9b-43ca-5d79-8c4b-1e967f899ad5`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_with_telemetry()\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:18.232942\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `resolve_data_directories`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:18.412615\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `ingest_data`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:18.639918\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `ingest_data`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:18.819808\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `resolve_data_directories`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:18.999054\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mPipeline run completed: `0e01cb9b-43ca-5d79-8c4b-1e967f899ad5`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_with_telemetry()\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:19.184851\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `ingest_data`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:19.358823\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `resolve_data_directories`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:19.533147\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mPipeline run completed: `0e01cb9b-43ca-5d79-8c4b-1e967f899ad5`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_with_telemetry()\u001b[0m]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RSS feeds added\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#########################################################\n",
    "# News Sources\n",
    "#########################################################\n",
    "\n",
    "# minimal rss list you can extend\n",
    "rss_feeds = [\n",
    "\t# --- Research ---\n",
    "\t\"https://export.arxiv.org/rss/cs.LG\",           # machine learning\n",
    "\t# \"https://export.arxiv.org/rss/cs.AI\",           # artificial intelligence\n",
    "\t# \"https://export.arxiv.org/rss/cs.CL\",           # computation & language\n",
    "\t# \"https://export.arxiv.org/rss/cs.CV\",           # computer vision\n",
    "\t# \"https://export.arxiv.org/rss/stat.ML\",         # statistical ML\n",
    "\t# \"https://export.arxiv.org/rss/eess.IV\",         # image/video systems (vision)\n",
    "\n",
    "\t# --- Major AI labs & research orgs ---\n",
    "\t\"https://openai.com/news/rss.xml\",\n",
    "\t# \"https://ai.googleblog.com/feeds/posts/default?alt=rss\",\n",
    "\t# \"https://deepmind.google/discover/blog/feed.xml\",     # DeepMind\n",
    "\t# \"https://research.ibm.com/blog/rss.xml\",              # IBM Research\n",
    "\t# \"https://blogs.microsoft.com/ai/feed/\",               # Microsoft AI blog\n",
    "\t# \"https://ai.meta.com/blog/rss.xml\",                   # Meta AI\n",
    "\t# \"https://huggingface.co/blog/feed.xml\",               # Hugging Face\n",
    "\n",
    "\t# --- Industry / analysis / newsletters ---\n",
    "\t# \"https://www.technologyreview.com/feed/\",             # MIT Tech Review\n",
    "\t# \"https://www.therundown.ai/feed\",                     # The Rundown AI\n",
    "\t# \"https://www.theneurondaily.com/feed\",                # The Neuron\n",
    "\t# \"https://www.semiaanalysis.com/feed\",                 # SemiAnalysis\n",
    "\t# \"https://www.oneusefulthing.org/feed\",                # Ethan Mollick\n",
    "\t# \"https://importai.substack.com/feed\",                 # Jack Clark‚Äôs Import AI\n",
    "\t# \"https://www.lesswrong.com/feed.xml\",                 # AI safety / rationalist posts\n",
    "\t# \"https://alignmentforum.org/feed.xml\",                # alignment research forum\n",
    "\n",
    "\t# --- Tech / product news outlets ---\n",
    "\t# \"https://techcrunch.com/tag/ai/feed/\",\n",
    "\t# \"https://www.theverge.com/rss/index.xml\",\n",
    "\t# \"https://venturebeat.com/category/ai/feed/\",\n",
    "\t# \"https://www.wired.com/feed/category/ai/latest/rss\",  # Wired AI\n",
    "\t# \"https://thenextweb.com/feed/\",                       # The Next Web\n",
    "\t# \"https://spectrum.ieee.org/feed\",                     # IEEE Spectrum (engineering AI)\n",
    "\n",
    "\t# --- Policy, ethics, society ---\n",
    "\t# \"https://hai.stanford.edu/rss.xml\",                   # Stanford HAI\n",
    "\t# \"https://www.brookings.edu/topic/artificial-intelligence/feed/\",  # Brookings AI policy\n",
    "\t# \"https://datainnovation.org/feed/\",                   # Center for Data Innovation\n",
    "\n",
    "\t# --- Optional fun / creative ---\n",
    "\t# \"https://towardsdatascience.com/feed\",                # Medium ML articles\n",
    "\t# \"https://www.kdnuggets.com/feed\",                     # data science & AI\n",
    "]\n",
    "\n",
    "\n",
    "# Parse RSS feeds and save to temp files\n",
    "print(f\"\\nParsing {len(rss_feeds)} RSS feeds...\")\n",
    "rss_temp_files = []\n",
    "\n",
    "for feed_url in rss_feeds:\n",
    "    try:\n",
    "        print(f\"  Parsing {feed_url}...\")\n",
    "        feed = feedparser.parse(feed_url)\n",
    "        \n",
    "        if not feed.entries:\n",
    "            print(f\"  ‚ö†Ô∏è  No entries found\")\n",
    "            continue\n",
    "        \n",
    "        # Format feed content\n",
    "        feed_text = [f\"RSS Feed: {feed.feed.get('title', feed_url)}\\n\"]\n",
    "        feed_text.append(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "        for entry in feed.entries[:10]:  # Get latest 10 entries\n",
    "            # Evaluate catchiness using LLM\n",
    "            title, summary = entry.get('title', 'No title'), entry.get('summary', entry.get('description', 'No summary available'))\n",
    "            prompt = f\"Rate this news catchiness (1-10). Catchiness = controversial + attention-capturing + timeless + major breakthrough.\\n\\nTitle: {title}\\nSummary: {summary[:500]}\\n\\nReturn ONLY integer 1-10.\"\n",
    "            \n",
    "            client = AsyncOpenAI(api_key=os.getenv(\"LLM_API_KEY\"), timeout=5.0)\n",
    "            resp = await client.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": prompt}], max_tokens=5)\n",
    "            score = int(resp.choices[0].message.content.strip())\n",
    "            \n",
    "            \n",
    "            feed_text.append(f\"\"\"\n",
    "                Title: {entry.get('title', 'No title')}\n",
    "                Published: {entry.get('published', entry.get('updated', 'N/A'))}\n",
    "                Link: {entry.get('link', 'N/A')}\n",
    "                Catchiness Score: {score}/10\n",
    "\n",
    "                Summary:\n",
    "                {entry.get('summary', entry.get('description', 'No summary available'))}\n",
    "\n",
    "                ---\n",
    "                \"\"\")\n",
    "        \n",
    "        # Save to temp file\n",
    "        temp_file = tempfile.NamedTemporaryFile(\n",
    "            mode='w', suffix='_rss.txt', delete=False, encoding='utf-8'\n",
    "        )\n",
    "        temp_file.write(\"\\n\".join(feed_text))\n",
    "        temp_file.close()\n",
    "        rss_temp_files.append(temp_file.name)\n",
    "        print(f\"  ‚úì Saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Failed: {e}\")\n",
    "\n",
    "# Add RSS feeds to cognee\n",
    "if rss_temp_files:\n",
    "    print(\"\\nAdding RSS feeds to cognee...\")\n",
    "    await cognee.add(rss_temp_files)\n",
    "    print(\"‚úì RSS feeds added\")\n",
    "    \n",
    "    # Cleanup\n",
    "    for temp_file in rss_temp_files:\n",
    "        try:\n",
    "            Path(temp_file).unlink()\n",
    "        except:\n",
    "            pass\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No RSS feeds were parsed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f83a8e",
   "metadata": {},
   "source": [
    "### Build & Enhance the Graph\n",
    "`cognify()` extracts entities/relations to build the knowledge graph.  \n",
    "Saves an initial HTML visualization.  \n",
    "`memify()` consolidates memory to strengthen connections.  \n",
    "Saves an enhanced HTML visualization for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32026b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m2025-11-13T16:23:19.733292\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mOntology file 'None' not found. No owl ontology will be attached to the graph.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:19.779455\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mPipeline run started: `520fb50a-9435-5383-be87-3c6dedd16fdf`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_with_telemetry()\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:19.970566\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `classify_documents`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:20.144092\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `check_permissions_on_dataset`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:20.333297\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mPipeline run started: `520fb50a-9435-5383-be87-3c6dedd16fdf`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_with_telemetry()\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:20.512001\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `classify_documents`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:20.690558\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `check_permissions_on_dataset`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:20.877491\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mPipeline run started: `520fb50a-9435-5383-be87-3c6dedd16fdf`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_with_telemetry()\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:21.062202\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `classify_documents`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:21.243825\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `check_permissions_on_dataset`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:21.450528\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAsync Generator task started: `extract_chunks_from_documents`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:21.656883\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAsync Generator task started: `extract_chunks_from_documents`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:21.889839\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAsync Generator task started: `extract_chunks_from_documents`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:22.084284\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `extract_graph_from_data`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:22.298623\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `extract_graph_from_data`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:23:22.497338\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `extract_graph_from_data`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.102304\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mLoaded JSON extension         \u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.139008\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'paper' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.141722\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'a lightweight cnn-attention-bilstm architecture for multi-class arrhythmia classification on standard and wearable ecgs' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.146230\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'accelerating training speed of tiny recursive models via curriculum guided adaptive recursion (cgar)' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.147494\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'learning the basis: a kolmogorov-arnold network approach embedding greens function priors (phykan)' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.147867\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'tabpfn-2.5: advancing the state of the art in tabular foundation models' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.148660\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'pegnet: a physics-embedded graph network for long-term stable multiphysics simulation' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.149199\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'fairplai: a human-in-the-loop approach to fair and private machine learning' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.149647\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'benevolent dictators? on llm agent behavior in dictator games' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.150195\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'macroscopic emission modeling of urban traffic using probe vehicle data: a machine learning approach' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.150890\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'gromov-wasserstein graph coarsening' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.151507\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'hey pentti, we did (more of) it!: a vector-symbolic lisp with residue arithmetic' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.151926\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'date' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.152321\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for '2025-11-13' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.152729\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'dataset' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.152988\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'cpsc 2018 dataset' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.153396\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'sudoku-extreme dataset' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.154694\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'benchmark' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.155514\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'tabarena benchmark' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.155744\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'probe vehicle data (u.s. urban areas)' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.155992\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'respiratory airflow and drug delivery datasets' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.156222\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'six large-scale datasets' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.156415\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'repository' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.156565\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'cgar github repository' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.156761\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'pegnet github repository' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.156953\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'fairplai github repository' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.157101\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'llm-abs github repository' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.157414\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'method' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.157768\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'lightweight cnn-attention-bilstm' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.158841\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'curriculum guided adaptive recursion (cgar)' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.159473\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'phykan (physics-informed kolmogorov-arnold network)' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.160335\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'tabpfn-2.5' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.160864\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'pegnet (physics-embedded graph network)' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.161264\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'fairplai' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.161523\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'llm-abs framework' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.161781\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'gromov-wasserstein graph coarsening' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.162041\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'concept' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.162502\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'vector-symbolic architecture with residue hyperdimensional computing and fhrrs' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:24:59.162960\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'macroscopic emission fundamental diagram (emfd)' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:02.953200\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `summarize_text`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.012766\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'article' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.015737\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'neuro drives national retail wins with chatgpt business' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.018221\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'fighting the new york times‚Äô invasion of user privacy' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.019587\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'gpt-5.1 instant and gpt-5.1 thinking system card addendum' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.020429\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'gpt-5.1: a smarter, more conversational chatgpt' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.022404\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'free chatgpt for transitioning u.s. servicemembers and veterans' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.023556\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'understanding prompt injections: a frontier security challenge' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.024845\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'notion‚Äôs rebuild for agentic ai: how gpt‚Äë5 helped unlock autonomous workflows' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.026072\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'from pilot to practice: how bbva is scaling ai across the organization' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.027525\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'introducing the teen safety blueprint' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.028878\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'ai progress and recommendations' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.031327\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'organization' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.032415\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'openai' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.033537\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'neuro' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.036068\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'the new york times' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.037112\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'notion' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.038153\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'bbva' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.039015\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'group' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.039930\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'u.s. servicemembers and veterans' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.041028\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'product' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.041557\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'chatgpt business' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.042072\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'chatgpt plus' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.042647\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'chatgpt enterprise' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.044416\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'feature' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.045078\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'custom gpts' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.045450\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'model' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.046285\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'gpt-5.1' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.047049\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'gpt-5' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.048535\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'gpt-5.1 instant' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.049586\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'gpt-5.1 thinking' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.050459\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'concept' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.051439\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'prompt injection' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.052269\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'user data privacy' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.052770\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'safety metrics' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.053355\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'autonomous agents' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.054123\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'policy' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.054687\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'teen safety blueprint' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.055262\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'ai progress' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.055853\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'date' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.058253\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for '2025-11-12' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.059536\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for '2025-11-10' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.060164\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for '2025-11-07' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:12.061078\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for '2025-11-06' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:15.351693\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `summarize_text`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:22.861671\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `add_data_points`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:25.860814\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `add_data_points`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:26.860323\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `add_data_points`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:27.033944\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `summarize_text`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:27.220406\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `extract_graph_from_data`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:27.408343\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAsync Generator task completed: `extract_chunks_from_documents`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:27.582006\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `check_permissions_on_dataset`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:27.755582\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `classify_documents`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:27.944944\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mPipeline run completed: `520fb50a-9435-5383-be87-3c6dedd16fdf`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_with_telemetry()\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:29.763606\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `add_data_points`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:29.941779\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `summarize_text`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:30.126369\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `extract_graph_from_data`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:30.317652\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAsync Generator task completed: `extract_chunks_from_documents`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:30.490973\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `check_permissions_on_dataset`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:30.679696\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `classify_documents`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:30.859419\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mPipeline run completed: `520fb50a-9435-5383-be87-3c6dedd16fdf`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_with_telemetry()\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.244583\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'subreddit' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.245881\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'r/openai' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.249454\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'post' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.250856\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'please, bring back the 24h limit for free users instead of the 5h limit.' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.251966\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'person' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.252943\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'u/maerulezok' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.253593\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'date' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.254251\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for '2025-11-13' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.255590\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'webpage' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.256511\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'please, bring back the 24h limit url' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.257657\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'organization' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.259398\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'openai' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.260550\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'product' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.262075\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'gpt-5.1' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.265283\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'concept' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.266058\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'free tier message limits change' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.266634\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'go' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.267840\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'mini version' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.270049\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'ai quality has quietly declined over the past year and it is hard not to ignore' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.270359\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'u/top-university-3832' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.270977\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'sora' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.271465\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'apob' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.272415\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'model regression or variability' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.273431\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'the cognitive chain of custody' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.274002\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'u/altruistic_log_7627' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.274554\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'cognitive governance' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.275139\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'constitutional law' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.324160\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'administrative law' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.341698\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'privacy and data law' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.362460\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'tort law' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.365299\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'human rights' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.366453\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'labor law' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.367020\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'the dark side of ai ‚Äî are we ready for its manipulative potential?' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.367883\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'u/one-direction-5341' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.368540\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'misinformation' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.368894\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'narrative control' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.369197\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'how to get gpt to stop mindlessly repeating its custom instructions back to me every message?' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.369448\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'u/ihateredditors111111' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.369746\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'custom instructions repetition issue' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.369999\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'voice mode issue' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.370216\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'your gpt-5.1 experiences?' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.370429\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'u/prestigiouspite' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.370975\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'how far do the included plus tokens go for coding with codex?' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.371777\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'u/luckecstatic9842' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.372146\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'plus plan' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.372387\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'codex' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.372726\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'how artists can protect their work from ai | dr. heather zheng | tedxchicago' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.373079\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'u/-rudoka-' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.383603\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'dr. heather zheng' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.385477\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'tedxchicago dr heather zheng youtube' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.386243\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'openai‚Äôs open-weight models are coming to the us military' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.386796\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'u/wiredmagazine' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.387367\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'wired article url' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.388413\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'us military' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.389029\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'deep research quota banner showing while i‚Äôm using agent mode' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.389476\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'u/hovercraftfar' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.390523\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'deep research quota banner issue' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.390888\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'agent mode' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.391942\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'thinking models fail on long tasks' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.392213\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'flyingchocolatecake' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.392554\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'gpt 5.1 issue' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.393122\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'joseph-siet' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.393789\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'does this feature exist?' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.394481\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'spie_09' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.394817\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'oh, we just gotta remove \"ok, enough\" to increase the output quality then ?' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.395180\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'scary_panic3165' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.395509\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'deezer/ipsos survey: music ai indistinguishability' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.395961\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'metaknowing' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.396265\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'deezer' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.396942\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'ipsos' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.398587\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'fact' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.399082\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for '97% cant tell ai vs human music' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.399446\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'ah, sweet manmade horrors' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.399719\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'openai publicly protests the new york times‚Äô demand for 20 million private user chats' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.400045\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'zshm' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.400901\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'the new york times' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.401241\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'dane stuckey' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.401883\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for '20 million chat records demand' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.402180\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'feature' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.402459\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'client-side encryption' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.402696\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'whats model Œ±?' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.402988\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'lightet3rnal' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.403234\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'finally no em dashesüò≠' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.403471\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'eulers_buttplug' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.403714\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'they copied the whole chatgpt answer and even kept the part where it offers to make it prettier.' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.404367\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'imfrom_mars_' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.405240\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'gpt-5.1 impressions: better clarity but limited problem-solving gains' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.405760\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'embarrassed_dish_265' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.406884\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'how?' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.407840\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'muskangulati_14' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.408193\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'chats getting laggy after a few days of use is one of the biggest most overlooked problems by openai' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.408484\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'pierrbourne' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.408737\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'chatgpt' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.409144\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'deep research' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.409401\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'model' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.409623\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'gpt-5' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.409875\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'issue' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.410125\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'thinking process timeout / crash' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.410700\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'dont know openai isnt releasing that product' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.410898\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'has anyone else run into this issue?' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.411191\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'u/shoobum-t' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.411413\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'u/kreativityyo' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.411659\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'openai agent builder' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.411873\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'service' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.412109\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'oauth' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.412724\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'database' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.413133\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'vercel' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.413444\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'stripe' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.413676\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'legacy models' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.413924\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'model 5.1' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.414115\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'action' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.414468\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'regenerate response' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.414752\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'r/chatgpt' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.415040\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'platform' in category 'classes'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.415285\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'chrome' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.415509\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'app' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.416033\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'mobile' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.416864\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'computer' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.418473\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'browser_cache' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:34.419635\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNo close match found for 'bug' in category 'individuals'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mOntologyAdapter\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:38.753171\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `summarize_text`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:25:55.879007\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task started: `add_data_points`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:26:01.024292\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `add_data_points`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:26:01.200404\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `summarize_text`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:26:01.385263\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `extract_graph_from_data`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:26:01.575981\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mAsync Generator task completed: `extract_chunks_from_documents`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:26:01.757806\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `check_permissions_on_dataset`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:26:01.933387\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mCoroutine task completed: `classify_documents`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_base\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:26:02.128509\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mPipeline run completed: `520fb50a-9435-5383-be87-3c6dedd16fdf`\u001b[0m [\u001b[0m\u001b[1m\u001b[34mrun_tasks_with_telemetry()\u001b[0m]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge graph created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m2025-11-13T16:26:02.673804\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mGraph visualization saved as /Users/lstromann/projects/community/experimental/ai_news_agent/simple_graph_visualization.html\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:26:02.674589\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mThe HTML file has been stored at path: /Users/lstromann/projects/community/experimental/ai_news_agent/simple_graph_visualization.html\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data visualized\n"
     ]
    }
   ],
   "source": [
    "# extract knowledge from the scraped data\n",
    "await cognee.cognify()\n",
    "print(\"Knowledge graph created.\")\n",
    "\n",
    "# visualize the knowledge graph\n",
    "simple_graph_visualization_path = str(Path.cwd() / \"simple_graph_visualization.html\")\n",
    "await cognee.visualize_graph(simple_graph_visualization_path)\n",
    "print(f\"Data visualized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd11a7",
   "metadata": {},
   "source": [
    "### Cross-Document Query\n",
    "Runs a GRAPH_COMPLETION search: *‚ÄúWhat were the main events in AI this week?‚Äù*  \n",
    "Returns a synthesized answer aggregated across Reddit + RSS sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90da3e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m2025-11-13T16:26:02.917917\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mGraph projection completed: 201 nodes, 566 edges in 0.01s\u001b[0m [\u001b[0m\u001b[1m\u001b[34mCogneeGraph\u001b[0m]\u001b[0m\n",
      "\n",
      "\u001b[2m2025-11-13T16:26:03.352478\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVector collection retrieval completed: Retrieved distances from 6 collections in 0.14s\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results: This week‚Äôs AI highlights: Reddit discussions on the dark side of AI and concerns about declining AI quality; broader discussion on AI progress and safety recommendations; Wired report that OpenAI‚Äôs open-weight models are being used by the US military; TEDx talk on how artists can protect work from AI; user complaints about missing OpenAI developer/product features; Deezer/Ipsos survey finding 97% of people can‚Äôt tell AI-generated music from human-made; Notion rebuilt its stack with GPT‚Äë5 to enable agentic/autonomous workflows; BBVA scaled ChatGPT Enterprise across the organization (20,000+ Custom GPTs) with reported efficiency gains (article published by OpenAI).\n"
     ]
    }
   ],
   "source": [
    "# demonstrate cross-document knowledge retrieval from multiple data sources\n",
    "results = await cognee.search(\n",
    "    query_text=\"What were the main events in the AI world this week?\",\n",
    "    query_type=cognee.SearchType.GRAPH_COMPLETION,\n",
    ")\n",
    "print(f\"Search results: {results[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a045a",
   "metadata": {},
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b099fb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m2025-11-13T16:29:55.796230\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mGraph visualization saved as /Users/lstromann/projects/community/experimental/ai_news_agent/fancy_graph_visualization.html\u001b[0m [\u001b[0m\u001b[1m\u001b[34mcognee.shared.logging_utils\u001b[0m]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DocumentChunk #1 Debug ===\n",
      "Text preview (first 200 chars): RSS Feed: cs.LG updates on arXiv.org\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "                Title: A Lightweight CNN-Attention-BiLSTM Architecture for Mult\n",
      "Catchiness match: <re.Match object; span=(394, 416), match='Catchiness Score: 4/10'>\n",
      "Catchiness extracted: 4\n",
      "Checking for 'Subreddit: r/' in text: False\n",
      "Checking for 'arxiv.org' in text: True\n",
      "Checking for 'RSS Feed:' in text: True\n",
      "üü¢ IDENTIFIED AS RESEARCH/ARXIV\n",
      "Research Feed: cs.LG updates on arXiv.org\n",
      "Flags - Reddit: False, Research: True, Other: False\n",
      "üü¢ Base color set to DARK GREEN (Research)\n",
      "Final color after brightness adjustment (catchiness=4.0): #478f47\n",
      "\n",
      "=== DocumentChunk #2 Debug ===\n",
      "Text preview (first 200 chars): RSS Feed: OpenAI News\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "                Title: Neuro drives national retail wins with ChatGPT Business\n",
      "               \n",
      "Catchiness match: <re.Match object; span=(314, 336), match='Catchiness Score: 6/10'>\n",
      "Catchiness extracted: 6\n",
      "Checking for 'Subreddit: r/' in text: False\n",
      "Checking for 'arxiv.org' in text: False\n",
      "Checking for 'RSS Feed:' in text: True\n",
      "üîµ IDENTIFIED AS OTHER/RSS\n",
      "RSS Feed: OpenAI News\n",
      "Flags - Reddit: False, Research: False, Other: True\n",
      "üîµ Base color set to DARK BLUE (Other/RSS)\n",
      "Final color after brightness adjustment (catchiness=6.0): #6b6bbb\n",
      "\n",
      "=== DocumentChunk #3 Debug ===\n",
      "Text preview (first 200 chars): \n",
      "                            Title: Please, bring back the 24h limit for free users instead of the 5h limit.\n",
      "                            Subreddit: r/OpenAI\n",
      "                            Author: u/maeru\n",
      "Catchiness match: <re.Match object; span=(458, 480), match='Catchiness Score: 4/10'>\n",
      "Catchiness extracted: 4\n",
      "Checking for 'Subreddit: r/' in text: True\n",
      "Checking for 'arxiv.org' in text: False\n",
      "Checking for 'RSS Feed:' in text: False\n",
      "üî¥ IDENTIFIED AS REDDIT\n",
      "Subreddit: r/OpenAI\n",
      "Flags - Reddit: True, Research: False, Other: False\n",
      "üî¥ Base color set to DARK RED (Reddit)\n",
      "Final color after brightness adjustment (catchiness=4.0): #ab4747\n",
      "\n",
      "=== DocumentChunk #4 Debug ===\n",
      "Text preview (first 200 chars): com/r/OpenAI/comments/1ovym40/deep_research_quota_banner_showing_while_im_using/\n",
      "                            Catchiness Score: 4/10\n",
      "\n",
      "                            Content:\n",
      "                            ht\n",
      "Catchiness match: <re.Match object; span=(109, 131), match='Catchiness Score: 4/10'>\n",
      "Catchiness extracted: 4\n",
      "Checking for 'Subreddit: r/' in text: True\n",
      "Checking for 'arxiv.org' in text: False\n",
      "Checking for 'RSS Feed:' in text: False\n",
      "üî¥ IDENTIFIED AS REDDIT\n",
      "Subreddit: r/OpenAI\n",
      "Flags - Reddit: True, Research: False, Other: False\n",
      "üî¥ Base color set to DARK RED (Reddit)\n",
      "Final color after brightness adjustment (catchiness=4.0): #ab4747\n",
      "\n",
      "=== DocumentChunk #5 Debug ===\n",
      "Text preview (first 200 chars): Don't know OpenAI isn't releasing that product\n",
      "                            Subreddit: r/OpenAI\n",
      "                            Author: u/ShooBum-T\n",
      "                            Score: 0\n",
      "                    \n",
      "Catchiness match: <re.Match object; span=(334, 356), match='Catchiness Score: 6/10'>\n",
      "Catchiness extracted: 6\n",
      "Checking for 'Subreddit: r/' in text: True\n",
      "Checking for 'arxiv.org' in text: False\n",
      "Checking for 'RSS Feed:' in text: False\n",
      "üî¥ IDENTIFIED AS REDDIT\n",
      "Subreddit: r/OpenAI\n",
      "Flags - Reddit: True, Research: False, Other: False\n",
      "üî¥ Base color set to DARK RED (Reddit)\n",
      "Final color after brightness adjustment (catchiness=6.0): #bb6b6b\n",
      "\n",
      "============================================================\n",
      "DOCUMENT CHUNK SUMMARY\n",
      "============================================================\n",
      "Total DocumentChunks: 5\n",
      "üî¥ Reddit posts (DARK RED): 3\n",
      "üü¢ Research/arXiv (DARK GREEN): 1\n",
      "üîµ Other/RSS (DARK BLUE): 1\n",
      "‚ö†Ô∏è  Unidentified: 0\n",
      "IDs tracked - Reddit: 3, Research: 1, Other: 1\n",
      "============================================================\n",
      "\n",
      "Building entity-to-source mapping from edges...\n",
      "Entities connected to - Reddit: 109, Research: 33, Other: 36\n",
      "  üü¢ Entity 'a lightweight cnn-attention-bilstm architecture for multi-class arrhythmia classification on standard and wearable ecgs' colored green (Research)\n",
      "  üü¢ Entity 'accelerating training speed of tiny recursive models via curriculum guided adaptive recursion (cgar)' colored green (Research)\n",
      "  üü¢ Entity 'learning the basis: a kolmogorov-arnold network approach embedding greens function priors (phykan)' colored green (Research)\n",
      "  üü† Entity '2025-11-13' colored orange/red (Reddit)\n",
      "  üîµ Entity 'neuro drives national retail wins with chatgpt business' colored blue (Other)\n",
      "  üîµ Entity 'fighting the new york times‚Äô invasion of user privacy' colored blue (Other)\n",
      "  üîµ Entity 'gpt-5.1 instant and gpt-5.1 thinking system card addendum' colored blue (Other)\n",
      "  üü† Entity 'openai' colored orange/red (Reddit)\n",
      "  üü† Entity 'the new york times' colored orange/red (Reddit)\n",
      "\n",
      "üü† Reddit entities (ORANGE #FF4500): 105 / 165\n",
      "üü¢ Research entities (GREEN #00FF00): 30 / 165\n",
      "üîµ Other entities (BLUE #5C10F4): 30 / 165\n",
      "\n",
      "============================================================\n",
      "COLOR SCHEME SUMMARY\n",
      "============================================================\n",
      "üî¥ DocumentChunks (Reddit)   ‚Üí DARK RED #8B0000 (brightness by catchiness)\n",
      "üü† Entities (Reddit)         ‚Üí ORANGE/RED #FF4500\n",
      "üü¢ DocumentChunks (Research) ‚Üí DARK GREEN #006400 (brightness by catchiness)\n",
      "üü¢ Entities (Research)       ‚Üí GREEN #00FF00\n",
      "üîµ DocumentChunks (Other)    ‚Üí DARK BLUE #00008B (brightness by catchiness)\n",
      "üîµ Entities (Other)          ‚Üí BLUE #5C10F4\n",
      "============================================================\n",
      "\n",
      "Fancy graph visualized with custom catchiness coloring: /Users/lstromann/projects/community/experimental/ai_news_agent/fancy_graph_visualization.html\n"
     ]
    }
   ],
   "source": [
    "# generate the second graph visualization after memory enhancement using custom visual.py\n",
    "import importlib\n",
    "import visual\n",
    "importlib.reload(visual)  # Reload to pick up latest changes\n",
    "from visual import cognee_network_visualization\n",
    "from cognee.infrastructure.databases.graph import get_graph_engine\n",
    "\n",
    "# Get graph data from cognee\n",
    "graph_engine = await get_graph_engine()\n",
    "graph_data = await graph_engine.get_graph_data()\n",
    "\n",
    "# Use custom visualization with catchiness-based brightness\n",
    "fancy_graph_visualization_path = str(Path.cwd() / \"fancy_graph_visualization.html\")\n",
    "await cognee_network_visualization(graph_data, fancy_graph_visualization_path)\n",
    "print(f\"Fancy graph visualized with custom catchiness coloring: {fancy_graph_visualization_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
